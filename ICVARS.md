# Experiment on Intuitive Gesture Manipulation in Virtual Reality 

This is the subtitle of the paper, this document both explains and embodies the submission format for authors using Word
<br>
<br>

<font size="4"> Lee Jen Tun*  </font>

Japan Advanced Institute of Science and Technology. Ishikawa, Japan 923-1211

<font size="4"> R.P.C. Janaka Rajaphakase </font>

Tainan National University of Arts, Tainan, Taiwan

<font size="4"> Kazunori Miyata </font>

Japan Advanced Institute of Science and Technology. Ishikawa, Japan 923-1211
<br>
<br>

<font size="1">
手勢交互是現代虛擬實境 (VR) 用戶體驗的新型態輸入模式。目前的手勢交互分為攝影機生成與感測器生成兩種，對於兩種型態的手勢交互的差別在許多研究中並沒有被詳細的探討過。因此，我們設計了數種手勢交互並執行了一系列的用戶研究。
在實驗過程中，用戶可以直觀地通過不同的手勢與虛擬物件互動。我們透過 speed stacks 來比較各個裝置間的完成時間，發現到感測器生成的手勢交互完成的時間比攝影機生成的快速，用戶的評估結果也顯示感測器生成的手勢交互能夠提供較直覺式的操作。此外，我們還展示了其他的手勢交互方式，例如縮放物體、移動物體、可交互式的虛擬對象等。最後，我們通過用戶研究評估了兩種方式的性能和可用性。

Gesture interaction is a new type of input mode for modern virtual reality (VR) user experience. The current gesture interaction is divided into two types: camera generation and sensor generation. The difference between the two types of gesture interaction has not been discussed in detail in many studies. Therefore, we designed several gestural interactions and performed a series of user studies.
During the experiment, users can intuitively interact with virtual objects through different gestures. We used speed stacks to compare the completion time of each device, and found that the gesture interaction generated by the sensor completed faster than that generated by the camera. The user evaluation results also showed that the gesture interaction generated by the sensor can provide a more intuitive operate. In addition, we also show other gestural interaction methods, such as zooming objects, moving objects, interactive virtual objects, etc. Finally, we evaluate the performance and usability of both modalities through user research. 

**KEYWORDS**\
Freehand interactions, Intuitive manipulation, Virtual Reality, Embodied operation.

**CCS CONCEPTS •** Insert your first CCS term here • Insert your second CCS term here • Insert your third CCS term here

**Additional Keywords and Phrases:** Insert comma delimited author-supplied keyword list, Keyword number 2, Keyword number 3, Keyword number 4

**ACM Reference Format:**\
First Author's Name, Initials, and Last Name, Second Author's Name, Initials, and Last Name, and Third Author's Name, Initials, and Last Name. 2018. The Title of the Paper: ACM Conference Proceedings Manuscript Submission Template: This is the subtitle of the paper, this document both explains and embodies the submission format for authors using Word. In Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03-05, 2018, Woodstock, NY. ACM, New York, NY, USA, 10 pages. NOTE: This block will be automatically generated when manuscripts are processed after acceptance.
</font>
<br>
<br>

### 1 INTRODUCTION
虛擬現實 (VR) 技術已廣泛應用於各個領域，包括教育[1, 2]、運動[3, 4]、和娛樂[5, 6]。與虛擬內容的交互在大多數這些VR體驗中扮演著至關重要的角色。作為與現實世界中物件操作的主要方式，手勢已被廣泛接受為與虛擬實境 (VR) 內容交互的直觀方法 [7, 8]。手勢可進一步分為靜態手勢與持續性手勢兩種，靜態手勢最常用於手勢交互的應用當中，例如碰觸 [9], 抓握 [10]跟猜拳 [11]。動態手勢 [12] 代表著現實世界當中最常使用到手勢，例如撫摸著寵物的頭、跟朋友說再見跟拍打背部等等...。這種徒手交互極大地提高了 VR 體驗中交互的沈浸感。

Virtual reality (VR) technology has been widely used in various fields, including education [1, 2], sports [3, 4], and entertainment [5, 6]. Interaction with virtual content plays a crucial role in most of these VR experiences. As the primary way of manipulating objects in the real world, gestures have been widely accepted as an intuitive method for interacting with virtual reality (VR) content [7, 8]. Gestures can be further classified into static gestures and persistent gestures. Static gestures are most commonly used in gesture interaction applications, such as touching [9], grasping [10] and guessing [11]. Dynamic gestures [12] represent the most frequently used gestures in the real world, such as stroking a pet's head, saying goodbye to a friend, slapping the back, etc.... This freehand interaction greatly increases the immersion of the interaction in a VR experience. 

以往的大多數工作都專注於裝置的開發[13, 14]與手勢的識別[15, 16]，無法涵蓋我們在日常生活中使用的基於手勢交互的複雜性和多樣性[17] (GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications)。現實世界互動的過程中，是在視野之內或者視野之外的互動也不同。最常見的例子是手眼協調； 然而，即使你閉著眼睛移動你的手臂，本體感受也會以運動信號的複制形式提供信息 [18](The VR Book)。 此外，當交互的對象需要實際觸碰時，攝影機的演算法要辨識出手的正確形狀也更加困難。用戶對於不同種類的交互任務和基於何種類型的手勢交互需要更深入的了解。

Most previous work has focused on device development [13, 14] and gesture recognition [15, 16] and cannot cover the complexity and variety of gesture-based interactions we use in our daily lives [17] (GesturAR: An Authoring System for Creating Freehand Interactive Augmented Reality Applications). In the process of interaction in the real world, the interaction within the field of view or outside the field of view is also different. The most common example is hand-eye coordination; however, proprioception provides information in the form of replicas of motion signals, even when you move your arms with your eyes closed [18] (The VR Book). Also, it is more difficult for the camera's algorithms to recognize the correct shape of the hand when the interacting objects need to be physically touched. Users need a deeper understanding of different kinds of interaction tasks and what types of gesture-based interactions are based.

對於基於攝影機與基於感測器兩種方式都能實時檢測手勢輸入以跟虛擬對象達到相應的響應。然而，對於哪一種任務適合基於何種感測方式的手勢交互需要專業知識。另一方面，具體的現實任務演示需要直觀的方式才能與虛擬內容達到相互同步的效果 [19]。通過幾個示例的演示，用戶可以了解到兩種裝置的優缺點，而無需了解底層細節。非專業的開發者也可以根據他們的應用和周圍環境來挑選相應的設備來實現手勢交互。

For both camera-based and sensor-based methods, gesture input can be detected in real time to achieve corresponding responses to virtual objects. However, expert knowledge is required for which task is suitable for gesture interaction based on which sensing modalities. On the other hand, concrete real-world task presentations require an intuitive way to achieve mutual synchronization with virtual content [19]. Demonstrations of several examples allow users to see the advantages and disadvantages of both devices without needing to understand the low-level details. Non-professional developers can also choose the appropriate device to implement gesture interaction according to their application and surrounding environment. 

我們比較目前最常見的設備, Oculus Quest2 [20], LeapMotion [21], ValveIndex [22], 與基於感測器的數據手套 [23], 開發一系列允許用戶通過手勢交互的應用 (圖 1a, 1b, 1c)。因此，用戶可以通過以上四種不同的裝置來操作不同的手勢交互任務，並執行一系列的用戶研究來得出裝置之間適合的應用場合。

We compare the most common devices today, Oculus Quest2 [20], LeapMotion [21], ValveIndex [22], with sensor-based data gloves [23], and develop a series of applications that allow users to interact through gestures (Fig. 1a, 1b, 1c). Therefore, the user can operate different gesture interaction tasks through the above four different devices, and perform a series of user studies to obtain suitable application scenarios between the devices.

### 2 RELATED WORKS

#### 2.1 Intuitive Interaction
直覺是一種認知過程，通常是無意識的，我們對直覺交互概念的理論方法可以發現它是基於經驗的[24 Intuition comes with experience]。
日常用品、傢俱和生活方式對於直覺的認知都是具有深度影響的。
直覺通常是無意識的，因此難以用語言描述，通常可以透過人們無意識的行為當中觀察出來 [Ag86; Bs03; BRBP90; DDA86; Fs87; KC02; LG97; NS84]。與有意識的思維處理模式相比直覺處理形式更快[Bs03; Sl83]，大部分的情況下是正確的，但並非絕對可靠 [Bs03]。基於這樣的理解，我們制定了直覺交互的定義：
直覺是一種認知過程，可以從過往的經驗中習得的行為，執行的當下是快速且無意識的，也難以透過語言描述出來。因此，人們直觀使用的操作方式是那些他們以前時常遇到過的情境。直覺交互同樣快速且無意識，因此人們可能無法解釋他們在直覺交互過程中是如何做出決定的 [Bl08; BPM02; BPM03a; BPM03b; BPM04a; BPM04b; BPM05; BPM07, Concepts of intuitive interaction]。
以前沒有設計師知道如何將熟悉的交互方式應用到虛擬實境以使其直觀。我們透過實驗確定了直觀交互和熟悉度之間的關係，以及場景設計的不同方面如何影響直觀交互 [Bl08; BPM02; BPM03a; BPM03b; BPM04b; BPM05; BPM07; BPM09] 我們研究的主要發現是：對相似特徵的熟悉的用戶比對相關特徵熟悉程度較低的用戶在完成任務上更快速、更能快速適應地執行交互任務。

Intuition is a cognitive process, usually unconscious, and our theoretical approach to the concept of intuitive interaction can find it to be based on experience [24 Intuition comes with experience].
Everyday objects, furniture, and lifestyles have a profound impact on intuitive perception.
Intuitions are often unconscious and therefore difficult to describe in words, and can often be observed through people's unconscious behavior [Ag86; Bs03; BRBP90; DDA86; Fs87; KC02; LG97; NS84]. The intuitive processing form is faster than the conscious thought processing mode [Bs03; Sl83], which is true in most cases, but not infallible [Bs03]. Based on this understanding, we formulate the definition of intuitive interaction:
Intuition is a cognitive process, a behavior that can be learned from past experience, executed in the present moment quickly and unconsciously, and difficult to describe in words. Therefore, people intuitively use manipulations in situations they have often encountered before. Intuitive interactions are also rapid and unconscious, so people may not be able to explain how they make decisions during intuitive interactions [B108; BPM02; BPM03a; BPM03b; BPM04a; BPM04b; BPM05; BPM07, Concepts of intuitive interaction].
No designer before knew how to apply familiar interactions to virtual reality to make it intuitive. We experimentally determined the relationship between intuitive interaction and familiarity, and how different aspects of scene design affect intuitive interaction [Bl08; BPM02; BPM03a; BPM03b; BPM04b; BPM05; BPM07; BPM09] The main findings of our study are: Familiar users with similar features are faster at completing tasks and more adaptable to perform interactive tasks than users with less familiarity with related features. 
#### 2.2 Intuition Experiment
直覺實驗在人機交互領域被提出作為一種測試直觀操作的方法，Stefan Brandenburg 和 Katharina Sachse [Intuition comes with experience] 設計了一個實驗環境是，他們製作了一個多點觸控桌面（約 80 cm x 105 cm）。介面包含了一個工作區域，一個文字敘述區域，一個開始按鈕和三個任務物件被放置在工作區域的右邊。實驗的任務是參加者對物件透過手勢執行三種不同的操作(旋轉、剪切、縮放)。
他們評估了兩個不同的時間，即首次點擊時間 (TFC) 和總任務時間 (TTT)。 TFC 代表從按下開始按鈕到開始執行手勢的延遲，而 TTT 代表從按下開始按鈕到成功完成手勢任務的整體時間。數據顯示，隨著時間的推移與經驗的累積，受試者在手勢執行所需的時間 (TTT) 和他們的初始反應時間 (TFC) 方面變得更快。

Intuition experiments have been proposed in the field of human-computer interaction as a way to test intuitive operation. Stefan Brandenburg and Katharina Sachse [Intuition comes with experience] designed an experimental environment in which they made a multi-touch desktop (about 80 cm x 105 cm). The interface consists of a work area, a text description area, a start button and three task objects are placed on the right side of the work area. The task of the experiment was for the participants to perform three different actions (rotation, shearing, scaling) on the object through gestures.
They evaluated two different times, time to first click (TFC) and total task time (TTT). TFC represents the delay from pressing the start button to starting the gesture, while TTT represents the overall time from pressing the start button to successfully completing the gesture task. The data showed that over time and with experience, subjects became faster in terms of time required to perform gestures (TTT) and their initial reaction time (TFC).

---
Tatsuya Iwaki, Naoki Nakamura 和 Katsuo Inoue [Basic study of the intuitive feel for user interface using the serial reaction time task.]製作了一個陣列式的按鈕顯示儀，參與者被要求盡可能快速地按下出現圖案的按鈕，在每次試驗中，測量從圖案顯示到按鈕按下的反應時間。之後，參與者以 9 分李克特式量表對反應的認知方面的直觀感覺進行評分。並在一週後再次執行相同的實驗。從他們的結果得知，參與者對任務反應的直覺感覺自評分數隨著反應時間的變化而增加。這意味著參與者覺得隨著內隱學習的進步，任務反應變得更加直觀。基於這些結果，人們認為從學習中獲得的知識是產品直觀操作的基礎。

Tatsuya Iwaki, Naoki Nakamura and Katsuo Inoue [Basic study of the intuitive feel for user interface using the serial reaction time task.] built an array of button displays in which participants were asked to press a patterned button as quickly as possible , and in each trial, measure the reaction time from pattern display to button press. Afterwards, participants rated the intuitive perception of the cognitive aspects of the response on a 9-point Likert scale. and perform the same experiment again a week later. From their results, participants' self-rated scores for their intuitive sense of response to the task increased with response time. This meant that participants felt that task responses became more intuitive as implicit learning progressed. Based on these results, it is believed that the knowledge gained from learning is the basis for intuitive operation of the product.

---
Alethea L. Blackler, Doug Mahar 跟 Vesna Popovic
[Empirical investigations into intuitive interaction: A summary]在他們其中一項實驗是要求使用執行兩個動作，第一個動作是使用相機的變焦功能在自動對焦模式下拍攝一張照片。
第二個動作，找到並且刪掉剛剛拍攝的照片。搜索存儲在相機中的指定圖像並放大它。結果表明，對於曾經使用過相似功能產品的參與者能夠更快、更直觀地完成操作。且產品直觀的首次使用結果尤為重要，因為在首次操作的情況下，參與者還不清楚指定功能在何處，但他們在第一次探索時就正確地完成任務。這些不是物理可供性，在多數情況下是很難預測到的特徵，因此參與者只能根據過往曾經操作過的類似特徵來採取行動。因此，他們的結果支持了這樣一種觀點，即在產品中具有類似的操作或者相同的文字介紹較能讓人們第一次直觀地使用它。 

Alethea L. Blackler, Doug Mahar and Vesna Popovic
[Empirical investigations into intuitive interaction: A summary] In one of their experiments they were asked to perform two actions, the first action being to use the camera's zoom function to take a picture in autofocus mode.
The second action, find and delete the photo just taken. Searches for the specified image stored in the camera and zooms in on it. The results showed that participants who had used similar functional products were able to complete the operation faster and more intuitively. And the intuitive first-use results of the product are especially important, because in the case of the first operation, the participants did not know where the specified function was, but they completed the task correctly on the first exploration. These are not physical affordances, and in most cases are characteristics that are difficult to predict, so participants can only act on similar characteristics that they have manipulated in the past. Therefore, their results support the idea that having a similar action or the same text description in a product is more intuitive for people to use it for the first time.

---
Despina Michael-Grigoriou, Panayiotis Yiannakou 跟 Maria Christofi [Intuitive Interaction for Exploring Human Anatomy in a VR Setup] 進行了組間研究，22 名參與者被平均分配在兩個組別。第一組"VR 組"的參與者透過VR 手勢操控的方式探索骨骼系統，而第二組"SP 組"(slide-show presentation)的參與者則通過幻燈片演示探索人體解剖結構。以上兩種方法可以找到完全相同的信息。
兩組的參與者都在執行實驗之前都先寫了一份總分十分的預測試知識問卷（pre-KT），執行完實驗之後再寫一份同樣的知識問卷 (post-KT)。 
此外，他們在實驗後給 VR 組的參與者發放了另一份問卷(5 點李克特量表)，是用來評估手部動作識別技術的問卷。 
從他們研究的結果顯示，在第一份問卷當中，從 pre-KT 跟 post_KT的數據分析可以了解到， VR 技術相對於幻燈片演示方法具有優勢。
第二份問卷，VR組的參與者對於透過手勢操作所帶來的沈浸感，系統的認知感跟操作輕鬆程度給予了非常高的評分。

Despina Michael-Grigoriou, Panayiotis Yiannakou and Maria Christofi [Intuitive Interaction for Exploring Human Anatomy in a VR Setup] conducted a between-group study with 22 participants equally divided between the two groups. Participants in the first "VR group" explored the skeletal system through VR gesture manipulation, while the second "SP group" (slide-show presentation) explored the human anatomy through a slide-show presentation. The above two methods can find the exact same information.
Participants in both groups wrote a pre-test knowledge questionnaire (pre-KT) with a total score of 10 before performing the experiment, and then wrote the same knowledge questionnaire (post-KT) after performing the experiment.
In addition, they gave participants in the VR group another questionnaire (5-point Likert scale) after the experiment, which was used to assess hand motion recognition techniques.
From the results of their research, in the first questionnaire, it can be seen from the data analysis of pre-KT and post_KT that VR technology has advantages over the slide presentation method.
In the second questionnaire, the participants in the VR group gave very high scores to the sense of immersion brought by gesture operation, the cognitive sense of the system and the ease of operation. 
#### 2.3 Relation between EEG and Intuition
